{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# Open the file for reading\n",
    "with open('output_file4.txt', 'r') as file:\n",
    "    # Read the text from the file\n",
    "    text = file.read()\n",
    "    \n",
    "# Convert the text to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove unwanted characters (punctuation marks and numbers)\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Perform stemming and lemmatization\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Open a new file for writing\n",
    "with open('output3.txt', 'w') as file:\n",
    "    # Write the preprocessed text to the file\n",
    "    file.write(' '.join(lemmatized_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open('bible.csv', newline='') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Read the text from the CSV file\n",
    "    text = ''\n",
    "    for row in reader:\n",
    "        text += ' '.join(row) + '\\n'\n",
    "\n",
    "# Convert the text to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove unwanted characters (punctuation marks and numbers)\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Perform stemming and lemmatization\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Open a new CSV file for writing\n",
    "with open('output_file.csv', 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the preprocessed text to the CSV file\n",
    "    writer.writerow(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read input text from file and add spaces after periods\n",
    "with open('3919_dho.txt', 'r') as input_file:\n",
    "    input_text = input_file.read().replace('\\n', '')\n",
    "    output_text = input_text.replace('.', '. ')\n",
    "\n",
    "# Define a regular expression to match digits\n",
    "digits_re = re.compile(r'\\d+')\n",
    "\n",
    "# Define a function to add spaces between digits and non-digits\n",
    "def add_spaces(match):\n",
    "    return f' {match.group(0)} '\n",
    "\n",
    "# Replace digits with spaced digits\n",
    "spaced_text = digits_re.sub(add_spaces, output_text)\n",
    "\n",
    "# Use regular expression to split text at number boundaries\n",
    "output_sentences = re.split(r'\\b\\d+\\b', spaced_text)\n",
    "\n",
    "# Write output sentences to file\n",
    "with open('output_file4.txt', 'w') as output_file:\n",
    "    for sentence in output_sentences:\n",
    "        output_file.write(sentence.strip() + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('3919_dho.txt', 'r') as input_file:\n",
    "    input_text = input_file.read().replace('\\n', '')\n",
    "    output_text = input_text.replace('.', '. ')\n",
    "    \n",
    "with open('output_file.txt', 'w') as output_file:\n",
    "    output_file.write(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a regular expression to match digits\n",
    "digits_re = re.compile(r'\\d+')\n",
    "\n",
    "# Define a function to add spaces between digits and non-digits\n",
    "def add_spaces(match):\n",
    "    return f' {match.group(0)} '\n",
    "\n",
    "# Open the input and output files\n",
    "with open('output_file.txt', 'r') as input_file, open('output_file2.txt', 'w') as output_file:\n",
    "    # Read in the input file\n",
    "    input_text = input_file.read()\n",
    "\n",
    "    # Replace digits with spaced digits\n",
    "    spaced_text = digits_re.sub(add_spaces, input_text)\n",
    "\n",
    "    # Write the spaced text to the output file\n",
    "    output_file.write(spaced_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read input sentence from file\n",
    "with open('output_file2.txt', 'r') as input_file:\n",
    "    input_sentence = input_file.read()\n",
    "\n",
    "# Use regular expression to split sentence at number boundaries\n",
    "output_sentences = re.split(r'\\b\\d+\\b', input_sentence)\n",
    "\n",
    "# Write output sentences to file\n",
    "with open('output_file3.txt', 'w') as output_file:\n",
    "    for sentence in output_sentences:\n",
    "        output_file.write(sentence.strip() + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
